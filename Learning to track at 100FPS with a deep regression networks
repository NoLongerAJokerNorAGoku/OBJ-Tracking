source：：https://blog.csdn.net/autocyz/article/details/52648776
这篇博文对原文有比较好的解读。
需要注意和补充的地方如下：

1）输入与输出
输入： 
在第t-1帧中，假设目标所在位置为（cx,cy），其大小为（w,h），则提取一块大小为（2w,2h）的图像块输入到CNN中。 
在第t帧中，也以（cx,cy）为中心，提取大小为（kw,kh）的图像块，输入到CNN中。k是一个先验的参数，作者的全部研究中均设为了2，我个人认为这个设定有提升的空间。
输出： 
输出目标在第t帧中左上角和右下角的坐标。

2）网络结构
前5层用的的是CaffeNet的前五层（caffenet： 
https://github.com/BVLC/caffe/tree/master/models/bvlc_reference_caffenet）给的卷积网络。
并在imagenet上进行了预训练。也正因为所有的训练都是离线的，预先做好的，该算法速度快；
之后是3层全连接层，每层都有4096个结点；
最后是一个只有四个结点的输出层，用于输出目标左上、右下的坐标。

3）目标区域预测
作者通过下述研究得出结论，使用拉氏变换比高丝变换对Tracker有更好的贡献。具体而言，就是更关心motion较小的区域，并裁减拉氏分布中概率密度较大的区域（即目标出现可能更大的区域）。
对于跟踪问题，一般的当前帧目标的位置和尺度都与上一帧是有关系，这个关系到底是怎么样的暂时没人分析过，作者通过对视频序列中的groundtruth进行研究发现，当前帧目标的位置和尺度变化与上一帧的目标存在着某种分布关系，具体分析如下： 
c′x=cx+w⋅Δx
c′y=cy+h⋅Δy
w′=w⋅γwh′=h⋅γh
 
上述描述中，cx,cy,w,h
代表的是上一帧目标的位置和大小，c′x,c′y,w′,h′
代表当前帧目标的位置和大小，Δx,Δy,γw,γh
代表目标位置和大小变化的程度（比例）。 
上述公式可以写成如下形式： 
Δx=（c′x−cx）/w
Δy=（c′y−cy）/y
γw=w′/wγh=h′/h
 
作者通过实验发现，这四个变化因子都符合laplace分布，laplace分布的概率密度函数为： 
f(x|μ,b)=12bexp(−|x−μ|b)
其中μ是位置参数，b是尺度参数，这个分布的期望E(x)=μ，方差为D(x)=2b^2。

4）训练
作者采用的训练方法是动态的视频+更大量的静态图片。
采用ALOV300++中的307段视频以及ImageNet Detection Challenge中的239，283副静态图片。
其对标有groundtruth的图片随机进行位置和尺度的变换，变换的参数也符合上述的laplace分布 。
